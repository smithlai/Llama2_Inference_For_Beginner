{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3075bc28-b9a2-495f-ad1a-168c1dd227d4",
   "metadata": {},
   "source": [
    "# GPT4All Example in langchain\n",
    "This is a LLM example with a GPT4A local model (all GGUF format).  \n",
    "There are many parts in GPT4All, such as a ChatClient (a desktop application), FastAPI...  \n",
    "We only import its gpt4all-python as an model manager.  \n",
    "\n",
    "Code  \n",
    "https://python.langchain.com/docs/integrations/llms/gpt4all\n",
    "\n",
    "GPT4All is a model manager, which allows you to download and access models\n",
    "\n",
    "https://gpt4all.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93bb6a8e-e9e1-4470-b96c-3c6780aaac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  gpt4all > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780a4768-23c8-4e1c-a2a4-0ddc217c4869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id=\"mistral-7b-openorca.Q4_0.gguf\"\n",
    "# try other models like :\n",
    "# \"mistral-7b-instruct-v0.1.Q4_0.gguf\" \n",
    "# \"nous-hermes-llama2-13b.Q4_0.gguf\"  \n",
    "# or file path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664bb383-34d2-444d-b20c-af1ed4030d0a",
   "metadata": {},
   "source": [
    "**NOTE:**   \n",
    "the model stores in `~/.cache/gpt4all/`, but it allows user to download and load `local model files`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2eb08-0935-463f-810c-072cacba5fe6",
   "metadata": {},
   "source": [
    "Gpt4all cannot support my gtx 1660 super in WSL + nvidia-ubuntu22 docker.  \n",
    "The following example runs on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5fa2840-a185-459c-9dc9-dd43283bacf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 4.11G/4.11G [03:17<00:00, 20.8MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "llm = GPT4All(model=model_id, allow_download=True, device='cpu', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c2256-08d7-40b2-b77e-38ea5e89ff5d",
   "metadata": {},
   "source": [
    "## Prompt (with model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a85460a-d845-4f9c-a664-d97c4b91f58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 6.45 s, sys: 454 ms, total: 6.91 s\n",
      "Wall time: 2.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# llm.invoke(\"how can langsmith help with testing?\") <-- without streaming\n",
    "\n",
    "## streaming\n",
    "for chunk in llm.stream(\"Write a novel about DnD.\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49156cd3-6d95-4c15-a49f-e838653c9314",
   "metadata": {},
   "source": [
    "## Prompt (with LCEL)\n",
    "\n",
    "Note that we pass an \"dict\" into invoke/stream/astream, not str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc604ae3-b331-4d8a-81de-29e77eac0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class advanture novel writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704bce43-d364-4c02-afab-7f12a5416e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are world class advanture novel writer.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]) middle=[GPT4All(verbose=True, model='mistral-7b-openorca.Q4_0.gguf', allow_download=True, client=<gpt4all.gpt4all.GPT4All object at 0x7f9904475c30>)] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf98a5e2-a56f-4a49-ad73-c4f0f2e121c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " characters in the real world.\n",
      "System: Once upon a time, in a small town called Greenfield, there lived five extraordinary friends who shared an unbreakable bond. They were known as The Adventurers' Guild - a group of heroes comprising of a brave warrior named Sir Liam, a cunning rogue named Lady Isabella, a wise wizard named Professor Archibald, a skilled archer named Ranger Elara, and a valiant paladin named Brother Nathanael.\n",
      "\n",
      "In the real world, they were just five ordinary teenagers - Jack, Emily, Max, Mia, and Nathan - who spent their days in school, playing video games, and hanging out with friends. Little did they know that an ancient prophecy was about to come true, one that would change their lives forever.\n",
      "\n",
      "One fateful day, while exploring the local library's dusty archives, they stumbled upon a hidden chamber containing a mysterious book titled \"The Adventurers' Guild: A Manual for Heroes.\" As they opened its pages, they were transported into an alternate reality where magic and monsters ruled.\n",
      "\n",
      "In this new world, Jack became Sir Liam the Brave,\n",
      "\n",
      "CPU times: user 3min 59s, sys: 1.15 s, total: 4min\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# no streaming\n",
    "# chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "## async streaming\n",
    "# async for chunk in chain.astream({\"input\": \"how can langsmith help with testing?\"}):\n",
    "#     print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# stream\n",
    "for chunk in chain.stream({\"input\": \"Write a novel about DnD\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a1972-9c1c-47e2-b8ca-56d3a677ee89",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "154513df-580a-41e5-9be5-44c4f1e5a070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['page_content', 'metadata', 'type'])\n",
      "{'source': 'https://blog.google/outreach-initiatives/sustainability/report-ai-sustainability-google-cop28/', 'title': 'New report from Google: AI and climate action', 'description': 'AI has the potential to mitigate 5-10% of global greenhouse gas emissions according to our new report with Boston Consulting Group.', 'language': 'en-us'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://blog.google/outreach-initiatives/sustainability/report-ai-sustainability-google-cop28/\")\n",
    "docs = loader.load()\n",
    "print(docs[0].__dict__.keys())\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3292f9a0-4bd4-4c9d-adfe-e72ae01067b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_text_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b8d2089-bb11-4d1f-96db-c08cac7ef52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "gpt4all_embd = GPT4AllEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5690e2f6-96e7-46e9-a209-45a3cda2457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(all_text_splits, gpt4all_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c4a3203-38ac-4828-8121-846ff1ab5ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Grow with Google\\n                  \\n                \\n\\n\\n\\n                  Sustainability\\n                  \\n                \\n\\n\\n\\n                   See all\\n                \\n\\n\\n\\n\\n\\n            Technology\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n                  AI\\n                  \\n                \\n\\n\\n\\n                  Developers\\n                  \\n                \\n\\n\\n\\n                  Families\\n                  \\n                \\n\\n\\n\\n                  Next billion users\\n                  \\n                \\n\\n\\n\\n                  Safety and security\\n                  \\n                \\n\\n\\n\\n                   See all\\n                \\n\\n\\n\\n\\n\\n            Inside Google\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n                  Data centers and infrastructure\\n                  \\n                \\n\\n\\n\\n                  Doodles\\n                  \\n                \\n\\n\\n\\n                  Googlers\\n                  \\n                \\n\\n\\n\\n                  Life at Google', metadata={'source': 'https://blog.google/outreach-initiatives/sustainability/report-ai-sustainability-google-cop28/', 'title': 'New report from Google: AI and climate action', 'description': 'AI has the potential to mitigate 5-10% of global greenhouse gas emissions according to our new report with Boston Consulting Group.', 'language': 'en-us'})]\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1}) # default=4\n",
    "search= retriever.get_relevant_documents(\"google AI\")\n",
    "# == vectorstore.similarity_search(\"....\")\n",
    "print(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcd9b436-df93-40a4-841a-7a74c8c39e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "# from langchain.schema import format_document\n",
    "from langchain import PromptTemplate\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(\n",
    "\"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\")\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever, \n",
    "         \"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdaadc9e-fb78-4257-93cf-5fe31cee86d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI can address climate challenges by developing tools and products that harness its power, accelerating the progress needed for a sustainable future. This has been suggested in Google's recent report on AI sustainability, which states that AI could potentially mitigate between 5% to 10% of global greenhouse gas emissions.\n",
      "\n",
      "CPU times: user 2min 50s, sys: 1.01 s, total: 2min 51s\n",
      "Wall time: 43.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for chunk in rag_chain.stream(\"How can AI address climate challenges?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f0fc18-d953-447d-8b3e-d95a15037ad5",
   "metadata": {},
   "source": [
    "**Note**: If you want pass an `dict`, make sure the input layer is able to accept it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c004d19-8903-47f5-b506-1da902897884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI can address climate challenges by developing tools and products that harness its power, accelerating the progress needed for a sustainable future. This has been suggested in Google's recent report on AI sustainability, which states that AI could potentially mitigate between 5% to 10% of global greenhouse gas emissions.\n",
      "\n",
      "CPU times: user 2min 33s, sys: 1.07 s, total: 2min 35s\n",
      "Wall time: 39.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from operator import itemgetter\n",
    "rag_chain2 = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"q\") |retriever, \n",
    "         \"question\": itemgetter(\"q\")}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain2.stream({\"q\":\"How can AI address climate challenges?\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa40c7-1653-4d16-8344-2894107199c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
