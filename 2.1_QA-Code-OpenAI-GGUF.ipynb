{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a734497c-6379-4d65-a468-e3921f04fe13",
   "metadata": {},
   "source": [
    "# Deeplake on GPT and GGUF\n",
    "https://python.langchain.com/docs/use_cases/question_answering/how_to/code/code-analysis-deeplake\n",
    "\n",
    "This notebook demostrate the difference between ChatGPT and Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ccb3017-73c1-4910-b51e-33cb38b6ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai\n",
    "# !pip3 install openai tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653154b5-0b93-4e79-8b1e-adcf02662f9e",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154fde9b-bbae-4828-9602-e1ec560e94fd",
   "metadata": {},
   "source": [
    "### Load text into Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8bbdf3-40b6-4ab9-b21e-dae545f462c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 3317 docs from 1953 *.py\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "root_dir = \"mydata/langchain-sourcecode/libs\"\n",
    "\n",
    "docs = []\n",
    "files = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".py\") and \"*venv/\" not in dirpath:\n",
    "            try:\n",
    "                filepath=os.path.join(dirpath, file)\n",
    "                loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "                files.append(filepath)\n",
    "                docs.extend(loader.load_and_split())\n",
    "            except Exception as e:\n",
    "                pass\n",
    "print(f\"load {len(docs)} docs from {len(files)} *.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64718247-92b8-44d2-94f6-c4d949618040",
   "metadata": {},
   "source": [
    "### Split Documants\n",
    "#### CharacterTextSplitter VS RecursiveCharacterTextSplitter\n",
    "##### RecursiveCharacterTextSplitter\n",
    "> https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter\n",
    "> tries to split on them in order until the chunks are small enough. The default list is \\[\"\\n\\n\", \"\\n\", \" \", \"\"\\]\n",
    "\n",
    "##### CharacterTextSplitter\n",
    "> https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter\n",
    "> This is the simplest method. This splits based on characters (by default \"\\n\\n\") and measure chunk length by number of characters.\n",
    ">\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adb2c0c-a7ed-475b-966f-16df1319f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5972 documents after split\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"There are {len(split_docs)} documents after split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a232769-6188-4d61-a3c6-6d94181b0bad",
   "metadata": {},
   "source": [
    "### Make Vectorstore with deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b9907fa-f980-4413-b92d-4a506ed69800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data locally\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/activeloop_deeplake#deep-lake-locally\n",
    "def make_local_vectorstore(split_docs,embeddings, dataset_path=\"./.my_deeplake/\"):\n",
    "    from langchain.vectorstores import DeepLake\n",
    "    local_vectorstore=DeepLake.from_documents(split_docs, dataset_path=dataset_path, embedding=embeddings, overwrite=True) #, read_only=True\n",
    "    return local_vectorstore \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533e9384-8474-4d10-ace7-835d3ace47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data on activeloop hub\n",
    "def make_hub_vectorstore(split_docs,embedding, hub_name):\n",
    "    from langchain.vectorstores import DeepLake\n",
    "    activeloop_key=\"\"\n",
    "    username = \"\"\n",
    "    with open('./mydata/activeloop_key.txt', 'r') as file:\n",
    "        username = file.readline().strip()\n",
    "        activeloop_key = file.readline().strip()\n",
    "    \n",
    "    os.environ['ACTIVELOOP_TOKEN'] = activeloop_key\n",
    "\n",
    "    hub_vectorstore = DeepLake.from_documents(\n",
    "        split_docs, embeddings, dataset_path=f\"hub://{username}/{hub_name}\", runtime={\"tensor_db\": True} #, overwrite=True\n",
    "    )\n",
    "    return hub_vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c61bf-1dcd-4376-81ce-5a0e73011c74",
   "metadata": {},
   "source": [
    "# Inference test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287fc4a1-30b0-4ec4-b25a-ab93c35302d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our inference test function\n",
    "from timeit import default_timer as timer\n",
    "def inferenceQA(chat_model, vectorstore):\n",
    "    qa = ConversationalRetrievalChain.from_llm(chat_model, retriever=vectorstore.as_retriever())\n",
    "    questions = [\n",
    "        \"What is the class hierarchy?\",\n",
    "        \"What classes are derived from the Chain class?\",\n",
    "        \"What kind of retrievers does LangChain have?\",\n",
    "    ]\n",
    "    chat_history = []\n",
    "    qa_dict = {}\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"-> **Question**: {question} \\n\")\n",
    "        start=timer()\n",
    "        result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "        end=timer()\n",
    "        print(f\"**{int((end-start)*100)/100.0} secs**\\n\")\n",
    "        print(f\"**Answer**: {result['answer']} \\n\")\n",
    "        chat_history.append((question, result[\"answer\"]))\n",
    "        qa_dict[question] = result[\"answer\"]\n",
    "    qa_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724aa62-5f7c-4b36-9975-32d8cf484a71",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff8b9da-acd8-435a-93b0-a0451687dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key=\"\"\n",
    "with open('./mydata/openai_api_key.txt', 'r') as file:\n",
    "    openai_api_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85686f24-f19a-490b-ae00-40f1540a6153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='xx-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======= OpenAI Transformer==========\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "gpt_embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "gpt_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5057d9ea-d730-47ae-8666-0b35bfe22b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating embeddings: 100% 916/916 [11:05<00:00,  1.38it/s]\n",
      "100% 5972/5972 [00:04<00:00, 1267.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./.my_gpt_deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype       shape       dtype  compression\n",
      "  -------    -------     -------     -------  ------- \n",
      "   text       text      (5972, 1)      str     None   \n",
      " metadata     json      (5972, 1)      str     None   \n",
      " embedding  embedding  (5972, 1536)  float32   None   \n",
      "    id        text      (5972, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# gpt_vectorstore = make_hub_vectorstore(split_docs, gpt_embeddings,hub_name=\"my_gpt_deeplake\")\n",
    "gpt_vectorstore = make_local_vectorstore(split_docs, gpt_embeddings, dataset_path=\"./.my_gpt_deeplake/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6fd1295-24f5-4394-93cf-3ade8f9d53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= OpenAI Model==========\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chat_model = ChatOpenAI(openai_api_key=openai_api_key,model_name=\"gpt-3.5-turbo-0613\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a2b9c1-57f3-4a6b-9e4c-c2baa46fc00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What is the class hierarchy? \n",
      "\n",
      "**2.61 secs**\n",
      "\n",
      "**Answer**: The class hierarchy for the Memory module is as follows:\n",
      "\n",
      "BaseMemory --> BaseChatMemory --> <name>Memory\n",
      "\n",
      "The class hierarchy for the ChatMessageHistory module is as follows:\n",
      "\n",
      "BaseChatMessageHistory --> <name>ChatMessageHistory\n",
      "\n",
      "The class hierarchy for the Document Transformers module is as follows:\n",
      "\n",
      "BaseDocumentTransformer --> <name> \n",
      "\n",
      "-> **Question**: What classes are derived from the Chain class? \n",
      "\n",
      "**7.35 secs**\n",
      "\n",
      "**Answer**: The classes derived from the Chain class are:\n",
      "\n",
      "- APIChain\n",
      "- OpenAPIEndpointChain\n",
      "- AnalyzeDocumentChain\n",
      "- MapReduceDocumentsChain\n",
      "- MapRerankDocumentsChain\n",
      "- ReduceDocumentsChain\n",
      "- RefineDocumentsChain\n",
      "- StuffDocumentsChain\n",
      "- ConstitutionalChain\n",
      "- ConversationChain\n",
      "- ChatVectorDBChain\n",
      "- ConversationalRetrievalChain\n",
      "- FlareChain\n",
      "- ArangoGraphQAChain\n",
      "- GraphQAChain\n",
      "- GraphCypherQAChain\n",
      "- FalkorDBQAChain\n",
      "- HugeGraphQAChain\n",
      "- KuzuQAChain\n",
      "- NebulaGraphQAChain\n",
      "- NeptuneOpenCypherQAChain\n",
      "- GraphSparqlQAChain\n",
      "- HypotheticalDocumentEmbedder\n",
      "- LLMChain\n",
      "- LLMCheckerChain\n",
      "- LLMMathChain\n",
      "- LLMRequestsChain\n",
      "- LLMSummarizationCheckerChain\n",
      "- OpenAIModerationChain\n",
      "- NatBotChain\n",
      "- QAGenerationChain\n",
      "- QAWithSourcesChain\n",
      "- RetrievalQAWithSourcesChain\n",
      "- VectorDBQAWithSourcesChain\n",
      "- RetrievalQA\n",
      "- VectorDBQA\n",
      "- LLMRouterChain\n",
      "- MultiPromptChain\n",
      "- MultiRetrievalQAChain\n",
      "- MultiRouteChain\n",
      "- RouterChain\n",
      "- SequentialChain\n",
      "- SimpleSequentialChain\n",
      "- TransformChain \n",
      "\n",
      "-> **Question**: What kind of retrievers does LangChain have? \n",
      "\n",
      "**5.97 secs**\n",
      "\n",
      "**Answer**: The classes derived from the Chain class in the LangChain are:\n",
      "\n",
      "- APIChain\n",
      "- OpenAPIEndpointChain\n",
      "- AnalyzeDocumentChain\n",
      "- MapReduceDocumentsChain\n",
      "- MapRerankDocumentsChain\n",
      "- ReduceDocumentsChain\n",
      "- RefineDocumentsChain\n",
      "- StuffDocumentsChain\n",
      "- ConstitutionalChain\n",
      "- ConversationChain\n",
      "- ChatVectorDBChain\n",
      "- ConversationalRetrievalChain\n",
      "- FlareChain\n",
      "- ArangoGraphQAChain\n",
      "- GraphQAChain\n",
      "- GraphCypherQAChain\n",
      "- FalkorDBQAChain\n",
      "- HugeGraphQAChain\n",
      "- KuzuQAChain\n",
      "- NebulaGraphQAChain\n",
      "- NeptuneOpenCypherQAChain\n",
      "- GraphSparqlQAChain\n",
      "- HypotheticalDocumentEmbedder\n",
      "- LLMChain\n",
      "- LLMCheckerChain\n",
      "- LLMMathChain\n",
      "- LLMRequestsChain\n",
      "- LLMSummarizationCheckerChain\n",
      "- OpenAIModerationChain\n",
      "- NatBotChain\n",
      "- QAGenerationChain\n",
      "- QAWithSourcesChain\n",
      "- RetrievalQAWithSourcesChain\n",
      "- VectorDBQAWithSourcesChain\n",
      "- RetrievalQA\n",
      "- VectorDBQA\n",
      "- LLMRouterChain\n",
      "- MultiPromptChain\n",
      "- MultiRetrievalQAChain\n",
      "- MultiRouteChain\n",
      "- RouterChain\n",
      "- SequentialChain\n",
      "- SimpleSequentialChain\n",
      "- TransformChain \n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferenceQA(chat_model, gpt_vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a0ad6-7fec-445b-ba64-73b741164920",
   "metadata": {},
   "source": [
    "## Llama2 (GGUF on CTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9261b89-d521-4660-88c8-bc09dab66497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== GGUF =========\n",
    "from langchain.llms import CTransformers\n",
    "import os\n",
    "\n",
    "model_id=os.path.abspath('./models/Llama-2-7b-Chat-GGUF')\n",
    "\n",
    "# context_length must be > chunk_size=1000 of text_splitter\n",
    "# If context length is too short, the output would be poor.\n",
    "config = {'max_new_tokens': 2048, 'repetition_penalty': 1.05,'context_length':4096}\n",
    "# https://api.python.langchain.com/en/latest/llms/langchain.llms.ctransformers.CTransformers.html\n",
    "cTransformers_llm = CTransformers(model=model_id, model_file=\"llama-2-7b-chat.Q4_K_M.gguf\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa992969-c928-4fee-84d5-31424c93df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding data\n",
    "from mylib.MyModelUtils import MyModelUtils\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "llama2_embeddings=HuggingFaceEmbeddings(\n",
    "    model_name=os.path.abspath(\"./models/sentence-transformers/all-mpnet-base-v2\"), \n",
    "    model_kwargs={\"device\": MyModelUtils.device()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9314bf3d-eee5-4311-a64d-a2bcc6323294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating embeddings: 100% 916/916 [03:30<00:00,  4.35it/s]\n",
      "100% 5972/5972 [00:04<00:00, 1264.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./.my_llama2_deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      "   text       text      (5972, 1)     str     None   \n",
      " metadata     json      (5972, 1)     str     None   \n",
      " embedding  embedding  (5972, 768)  float32   None   \n",
      "    id        text      (5972, 1)     str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# llama2_vectorstore = make_hub_vectorstore(split_docs, llama2_embeddings,hub_name=\"my_llama2_deeplake\")\n",
    "llama2_vectorstore = make_local_vectorstore(split_docs, llama2_embeddings, dataset_path=\"./.my_llama2_deeplake/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5deb63d6-5d93-4fd1-b4c7-d2ecd00f0c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What is the class hierarchy? \n",
      "\n",
      "**212.45 secs**\n",
      "\n",
      "**Answer**:  The class hierarchy is: BaseDocumentTransformer --> <name> --> Document.\n",
      "Explanation: The class hierarchy shows the relationships between different classes in the langchain.document_transformers module. The topmost class is \"BaseDocumentTransformer\", which is a base class for all document transformers. Below it are various subclasses, each representing a different type of document transformer (e.g., BeautifulSoupTransformer, DoctranQATransformer, etc.). Finally, at the bottom of the hierarchy is the \"Document\" class, which represents a single document that can be transformed by one of the above classes. \n",
      "\n",
      "-> **Question**: What classes are derived from the Chain class? \n",
      "\n",
      "**283.88 secs**\n",
      "\n",
      "**Answer**:   The Chain class is the base class for many other classes in the LangChain project, including APIChain, OpenAPIEndpointChain, AnalyzeDocumentChain, MapReduceDocumentsChain, MapRerankDocumentsChain, ReduceDocumentsChain, RefineDocumentsChain, StuffDocumentsChain, ConstitutionalChain, ConversationChain, ChatVectorDBChain, ConversationalRetrievalChain, FlareChain, ArangoGraphQAChain, GraphQAChain, GraphCypherQAChain, FalkorDBQAChain, HugeGraphQAChain, KuzuQAChain, NebulaGraphQAChain, NeptuneOpenCypherQAChain, GraphSparqlQAChain, HypotheticalDocumentEmbedder, LLMChain, LLMCheckerChain, LLMMathChain, LLMRequestsChain, LLMSummarizationCheckerChain, OpenAIModerationChain, NatBotChain, create_citation_fuzzy_match_chain, create_extraction_chain, create_extraction_chain_pydantic, create_qa_with_sources_chain, create_qa_with_structure_chain, create_tagging_chain, create_tagging_chain_pydantic, QAGenerationChain, QAWithSourcesChain, RetrievalQAWithSourcesChain, VectorDBQAWithSourcesChain, MultiPromptChain, MultiRetrievalQAChain, MultiRouteChain, RouterChain, LLMRouterChain, SequentialChain, SimpleSequentialChain, create_sql_query_chain, transform.\n",
      "Of these, the following are explicitly listed as derived from the Chain class:\n",
      "* APIChain\n",
      "* OpenAPIEndpointChain\n",
      "* AnalyzeDocumentChain\n",
      "* MapReduceDocumentsChain\n",
      "* MapRerankDocumentsChain\n",
      "* ReduceDocumentsChain\n",
      "* RefineDocumentsChain\n",
      "* StuffDocumentsChain\n",
      "* ConstitutionalChain\n",
      "* ConversationChain\n",
      "* ChatVectorDBChain\n",
      "* ConversationalRetrievalChain\n",
      "* FlareChain\n",
      "* ArangoGraphQAChain\n",
      "* GraphQAChain\n",
      "* GraphCypherQAChain\n",
      "* FalkorDBQAChain\n",
      "* HugeGraphQAChain\n",
      "* KuzuQAChain\n",
      "* NebulaGraphQAChain\n",
      "* NeptuneOpenCypherQAChain\n",
      "* GraphSparqlQAChain\n",
      "* HypotheticalDocumentEmbedder\n",
      "* LLMChain\n",
      "* LLMCheckerChain\n",
      "* LLMMathChain\n",
      "* LLMRequestsChain\n",
      "* LLMSummarizationCheckerChain\n",
      "* OpenAIModerationChain\n",
      "* NatBotChain\n",
      "* create_citation_fuzzy_match_chain\n",
      "\n",
      "* create_extraction_chain\n",
      "\n",
      "* create_extraction_chain_pydantic\n",
      "\n",
      "* create_qa_with_sources_chain\n",
      "\n",
      "* create_qa_with_structure_chain\n",
      "\n",
      "* create_tagging_chain\n",
      "\n",
      "* create_tagging_chain_pydantic\n",
      "\n",
      "* QAGenerationChain\n",
      "\n",
      "* QAWithSourcesChain\n",
      "\n",
      "* RetrievalQAWithSourcesChain\n",
      "\n",
      "* VectorDBQAWithSourcesChain\n",
      "\n",
      "* MultiPromptChain\n",
      "\n",
      "* MultiRetrievalQAChain\n",
      "\n",
      "* MultiRouteChain\n",
      "\n",
      "* RouterChain\n",
      "\n",
      "* LLMRouterChain\n",
      "\n",
      "* SequentialChain\n",
      "\n",
      "* SimpleSequentialChain\n",
      "\n",
      "* create_sql_query_chain\n",
      "\n",
      "\n",
      "* transform. \n",
      "\n",
      "-> **Question**: What kind of retrievers does LangChain have? \n",
      "\n",
      "**356.21 secs**\n",
      "\n",
      "**Answer**:  LangChain has a wide variety of retrievers, including Arcee, Arxiv, Azure Cognitive Search, BM25, Chaindesk, ChatGPT plugin, Contextual Compression, DocArray, Elastic Search BM25, Ensemble, Google Cloud Enterprise Search, Google Vertex AI Search, Kay, Amazon Kendra, KNN, Llama Index, Merger, Metal, Milvus, Multi Query, Multi Vector, Parent Document, Pinecone Hybrid Search, PubMed, RePhrase Query, Remote Retriever, Self Query, SVM, Tavily Search API, Time Weighted Vector Store, Vespa, Weaviate Hybrid Search, Web Research, Wikipedia, and Zilliz. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What will happen if we use gpt-embedding vertorstore?\n",
    "inferenceQA(cTransformers_llm, gpt_vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "745aab7f-dd44-41d0-af57-f85e734b5eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What is the class hierarchy? \n",
      "\n",
      "**226.02 secs**\n",
      "\n",
      "**Answer**:  The class hierarchy for tools in the given code snippet is as follows:\n",
      "\n",
      "* `ToolMetaclass`: The base metaclass for all tools.\n",
      "* `BaseTool`: The base class for all tools.\n",
      "* `<name>Tool`: A tool with a specific name (e.g., `AINetworkTool`, `AIOwnerTool`, etc.).\n",
      "\n",
      "In this hierarchy, each tool inherits from the previous class in the chain, with the top-most class being `ToolMetaclass`. This allows for a standardized and predictable class structure across all tools. \n",
      "\n",
      "-> **Question**: What classes are derived from the Chain class? \n",
      "\n",
      "**199.49 secs**\n",
      "\n",
      "**Answer**: \n",
      "The derived classes are listed below along with their parent class, `Chain`, based on the provided code snippet:\n",
      "\n",
      "* `LLMChain` (Chain)\n",
      "* `MapReduceChain` (Chain)\n",
      "* `RouterChain` (Chain)\n",
      "\n",
      "Therefore, the answer is `LLMChain`, `MapReduceChain`, and `RouterChain`. \n",
      "\n",
      "-> **Question**: What kind of retrievers does LangChain have? \n",
      "\n",
      "**132.16 secs**\n",
      "\n",
      "**Answer**:  According to the provided code snippet, the following classes are derived from the Chain class:\n",
      "\n",
      "* LLMChain\n",
      "* MapReduceChain\n",
      "* RouterChain\n",
      "\n",
      "Unhelpful Answer: I don't know the answer to your question. The code snippet doesn't provide enough information to determine the derived classes. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferenceQA(cTransformers_llm, llama2_vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedb51d-6e63-43e5-b954-4da5da03040a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
