{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a734497c-6379-4d65-a468-e3921f04fe13",
   "metadata": {},
   "source": [
    "# Deeplake on GPT and GGUF\n",
    "https://python.langchain.com/docs/use_cases/question_answering/how_to/code/code-analysis-deeplake\n",
    "\n",
    "This notebook demostrate the difference between ChatGPT and Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ccb3017-73c1-4910-b51e-33cb38b6ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai\n",
    "# !pip3 install openai tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653154b5-0b93-4e79-8b1e-adcf02662f9e",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8bbdf3-40b6-4ab9-b21e-dae545f462c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 3317 docs from 1953 *.py\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "root_dir = \"mydata/langchain-sourcecode/libs\"\n",
    "\n",
    "docs = []\n",
    "files = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".py\") and \"*venv/\" not in dirpath:\n",
    "            try:\n",
    "                filepath=os.path.join(dirpath, file)\n",
    "                loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "                files.append(filepath)\n",
    "                docs.extend(loader.load_and_split())\n",
    "            except Exception as e:\n",
    "                pass\n",
    "print(f\"load {len(docs)} docs from {len(files)} *.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64718247-92b8-44d2-94f6-c4d949618040",
   "metadata": {},
   "source": [
    "### CharacterTextSplitter VS RecursiveCharacterTextSplitter\n",
    "#### RecursiveCharacterTextSplitter\n",
    "> https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter\n",
    "> tries to split on them in order until the chunks are small enough. The default list is \\[\"\\n\\n\", \"\\n\", \" \", \"\"\\]\n",
    "\n",
    "#### CharacterTextSplitter\n",
    "> https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter\n",
    "> This is the simplest method. This splits based on characters (by default \"\\n\\n\") and measure chunk length by number of characters.\n",
    ">\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adb2c0c-a7ed-475b-966f-16df1319f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2456, which is longer than the specified 2048\n",
      "Created a chunk of size 2222, which is longer than the specified 2048\n",
      "Created a chunk of size 3323, which is longer than the specified 2048\n",
      "Created a chunk of size 2719, which is longer than the specified 2048\n",
      "Created a chunk of size 2638, which is longer than the specified 2048\n",
      "Created a chunk of size 2405, which is longer than the specified 2048\n",
      "Created a chunk of size 3173, which is longer than the specified 2048\n",
      "Created a chunk of size 2573, which is longer than the specified 2048\n",
      "Created a chunk of size 2248, which is longer than the specified 2048\n",
      "Created a chunk of size 2286, which is longer than the specified 2048\n",
      "Created a chunk of size 2196, which is longer than the specified 2048\n",
      "Created a chunk of size 2175, which is longer than the specified 2048\n",
      "Created a chunk of size 3268, which is longer than the specified 2048\n",
      "Created a chunk of size 2800, which is longer than the specified 2048\n",
      "Created a chunk of size 2409, which is longer than the specified 2048\n",
      "Created a chunk of size 2244, which is longer than the specified 2048\n",
      "Created a chunk of size 2375, which is longer than the specified 2048\n",
      "Created a chunk of size 2682, which is longer than the specified 2048\n",
      "Created a chunk of size 2461, which is longer than the specified 2048\n",
      "Created a chunk of size 2279, which is longer than the specified 2048\n",
      "Created a chunk of size 2285, which is longer than the specified 2048\n",
      "Created a chunk of size 2580, which is longer than the specified 2048\n",
      "Created a chunk of size 2900, which is longer than the specified 2048\n",
      "Created a chunk of size 3653, which is longer than the specified 2048\n",
      "Created a chunk of size 2797, which is longer than the specified 2048\n",
      "Created a chunk of size 2657, which is longer than the specified 2048\n",
      "Created a chunk of size 2169, which is longer than the specified 2048\n",
      "Created a chunk of size 2621, which is longer than the specified 2048\n",
      "Created a chunk of size 2282, which is longer than the specified 2048\n",
      "Created a chunk of size 2324, which is longer than the specified 2048\n",
      "Created a chunk of size 2737, which is longer than the specified 2048\n",
      "Created a chunk of size 2183, which is longer than the specified 2048\n",
      "Created a chunk of size 2171, which is longer than the specified 2048\n",
      "Created a chunk of size 3094, which is longer than the specified 2048\n",
      "Created a chunk of size 2733, which is longer than the specified 2048\n",
      "Created a chunk of size 2787, which is longer than the specified 2048\n",
      "Created a chunk of size 2311, which is longer than the specified 2048\n",
      "Created a chunk of size 2811, which is longer than the specified 2048\n",
      "Created a chunk of size 2168, which is longer than the specified 2048\n",
      "Created a chunk of size 3466, which is longer than the specified 2048\n",
      "Created a chunk of size 3084, which is longer than the specified 2048\n",
      "Created a chunk of size 2226, which is longer than the specified 2048\n",
      "Created a chunk of size 2674, which is longer than the specified 2048\n",
      "Created a chunk of size 2587, which is longer than the specified 2048\n",
      "Created a chunk of size 2300, which is longer than the specified 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5853 documents after split\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2048, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"There are {len(split_docs)} documents after split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b9907fa-f980-4413-b92d-4a506ed69800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data locally\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/activeloop_deeplake#deep-lake-locally\n",
    "def make_local_vectorstore(split_docs,embeddings, dataset_path=\"./.my_deeplake/\"):\n",
    "    from langchain.vectorstores import DeepLake\n",
    "    local_vectorstore=DeepLake.from_documents(split_docs, dataset_path=dataset_path, embedding=embeddings, overwrite=True) #, read_only=True\n",
    "    return local_vectorstore \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533e9384-8474-4d10-ace7-835d3ace47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data on activeloop hub\n",
    "def make_hub_vectorstore(split_docs,embedding):\n",
    "    from langchain.vectorstores import DeepLake\n",
    "    activeloop_key=\"\"\n",
    "    username = \"\"\n",
    "    with open('./mydata/activeloop_key.txt', 'r') as file:\n",
    "        username = file.readline().strip()\n",
    "        activeloop_key = file.readline().strip()\n",
    "    \n",
    "    os.environ['ACTIVELOOP_TOKEN'] = activeloop_key\n",
    "\n",
    "    hub_vectorstore = DeepLake.from_documents(\n",
    "        split_docs, embeddings, dataset_path=f\"hub://{username}/langchain-code\", runtime={\"tensor_db\": True} #, overwrite=True\n",
    "    )\n",
    "    hub_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287fc4a1-30b0-4ec4-b25a-ab93c35302d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our inference test function\n",
    "from timeit import default_timer as timer\n",
    "def inferenceQA(chat_model, vectorstore):\n",
    "    qa = ConversationalRetrievalChain.from_llm(chat_model, retriever=vectorstore.as_retriever())\n",
    "    questions = [\n",
    "        \"What is the class hierarchy?\",\n",
    "        \"What classes are derived from the Chain class?\",\n",
    "        \"What kind of retrievers does LangChain have?\",\n",
    "    ]\n",
    "    chat_history = []\n",
    "    qa_dict = {}\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"-> **Question**: {question} \\n\")\n",
    "        start=timer()\n",
    "        result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "        end=timer()\n",
    "        print(f\"**{int((end-start)*100)/100.0} secs**\\n\")\n",
    "        print(f\"**Answer**: {result['answer']} \\n\")\n",
    "        chat_history.append((question, result[\"answer\"]))\n",
    "        qa_dict[question] = result[\"answer\"]\n",
    "    qa_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724aa62-5f7c-4b36-9975-32d8cf484a71",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff8b9da-acd8-435a-93b0-a0451687dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key=\"\"\n",
    "with open('./mydata/openai_api_key.txt', 'r') as file:\n",
    "    openai_api_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85686f24-f19a-490b-ae00-40f1540a6153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======= OpenAI Transformer==========\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "gpt_embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "gpt_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5057d9ea-d730-47ae-8666-0b35bfe22b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating embeddings: 100% 920/920 [11:12<00:00,  1.37it/s]\n",
      "100% 5853/5853 [00:04<00:00, 1276.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./.my_deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype       shape       dtype  compression\n",
      "  -------    -------     -------     -------  ------- \n",
      "   text       text      (5853, 1)      str     None   \n",
      " metadata     json      (5853, 1)      str     None   \n",
      " embedding  embedding  (5853, 1536)  float32   None   \n",
      "    id        text      (5853, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# gpt_vectorstore = make_hub_vectorstore(split_docs, gpt_embeddings)\n",
    "gpt_vectorstore = make_local_vectorstore(split_docs, gpt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6fd1295-24f5-4394-93cf-3ade8f9d53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= OpenAI Model==========\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chat_model = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43a2b9c1-57f3-4a6b-9e4c-c2baa46fc00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What is the class hierarchy? \n",
      "\n",
      "**3.08 secs**\n",
      "\n",
      "**Answer**: The class hierarchy for Memory is as follows:\n",
      "\n",
      "BaseMemory --> BaseChatMemory --> <name>Memory\n",
      "\n",
      "The class hierarchy for ChatMessageHistory is as follows:\n",
      "\n",
      "BaseChatMessageHistory --> <name>ChatMessageHistory\n",
      "\n",
      "The class hierarchy for Document Transformers is as follows:\n",
      "\n",
      "BaseDocumentTransformer --> <name> \n",
      "\n",
      "-> **Question**: What classes are derived from the Chain class? \n",
      "\n",
      "**7.63 secs**\n",
      "\n",
      "**Answer**: The following classes are derived from the Chain class:\n",
      "\n",
      "- APIChain\n",
      "- OpenAPIEndpointChain\n",
      "- AnalyzeDocumentChain\n",
      "- MapReduceDocumentsChain\n",
      "- MapRerankDocumentsChain\n",
      "- ReduceDocumentsChain\n",
      "- RefineDocumentsChain\n",
      "- StuffDocumentsChain\n",
      "- ConstitutionalChain\n",
      "- ConversationChain\n",
      "- ChatVectorDBChain\n",
      "- ConversationalRetrievalChain\n",
      "- FlareChain\n",
      "- ArangoGraphQAChain\n",
      "- GraphQAChain\n",
      "- GraphCypherQAChain\n",
      "- FalkorDBQAChain\n",
      "- HugeGraphQAChain\n",
      "- KuzuQAChain\n",
      "- NebulaGraphQAChain\n",
      "- NeptuneOpenCypherQAChain\n",
      "- GraphSparqlQAChain\n",
      "- HypotheticalDocumentEmbedder\n",
      "- LLMChain\n",
      "- LLMCheckerChain\n",
      "- LLMMathChain\n",
      "- LLMRequestsChain\n",
      "- LLMSummarizationCheckerChain\n",
      "- MapReduceChain\n",
      "- OpenAIModerationChain\n",
      "- NatBotChain\n",
      "- QAGenerationChain\n",
      "- QAWithSourcesChain\n",
      "- RetrievalQAWithSourcesChain\n",
      "- VectorDBQAWithSourcesChain\n",
      "- RetrievalQA\n",
      "- VectorDBQA\n",
      "- LLMRouterChain\n",
      "- MultiPromptChain\n",
      "- MultiRetrievalQAChain\n",
      "- MultiRouteChain\n",
      "- RouterChain\n",
      "- SequentialChain\n",
      "- SimpleSequentialChain\n",
      "- TransformChain \n",
      "\n",
      "-> **Question**: What kind of retrievers does LangChain have? \n",
      "\n",
      "**2.45 secs**\n",
      "\n",
      "**Answer**: The LangChain class is not defined in the given context. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferenceQA(chat_model, gpt_vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a0ad6-7fec-445b-ba64-73b741164920",
   "metadata": {},
   "source": [
    "## Llama2 (GGUF on CTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9261b89-d521-4660-88c8-bc09dab66497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== GGUF =========\n",
    "from langchain.llms import CTransformers\n",
    "import os\n",
    "\n",
    "model_id=os.path.abspath('./models/Llama-2-7b-Chat-GGUF')\n",
    "\n",
    "# context_length must be > chunk_size=1000 of text_splitter\n",
    "# If context length is too short, the output would be poor.\n",
    "config = {'max_new_tokens': 2048, 'repetition_penalty': 1.1,'context_length':4096}\n",
    "# https://api.python.langchain.com/en/latest/llms/langchain.llms.ctransformers.CTransformers.html\n",
    "cTransformers_llm = CTransformers(model=model_id, model_file=\"llama-2-7b-chat.Q4_K_M.gguf\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa992969-c928-4fee-84d5-31424c93df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding data\n",
    "from mylib.MyModelUtils import MyModelUtils\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "llama2_embeddings=HuggingFaceEmbeddings(\n",
    "    model_name=os.path.abspath(\"./models/sentence-transformers/all-mpnet-base-v2\"), \n",
    "    model_kwargs={\"device\": MyModelUtils.device()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9314bf3d-eee5-4311-a64d-a2bcc6323294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating embeddings: 100% 920/920 [03:27<00:00,  4.44it/s]\n",
      "100% 5853/5853 [00:04<00:00, 1284.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./.my_deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      "   text       text      (5853, 1)     str     None   \n",
      " metadata     json      (5853, 1)     str     None   \n",
      " embedding  embedding  (5853, 768)  float32   None   \n",
      "    id        text      (5853, 1)     str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# llama2_vectorstore = make_hub_vectorstore(split_docs, llama2_embeddings)\n",
    "llama2_vectorstore = make_local_vectorstore(split_docs, llama2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb63d6-5d93-4fd1-b4c7-d2ecd00f0c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What is the class hierarchy? \n",
      "\n",
      "**220.42 secs**\n",
      "\n",
      "**Answer**:  The class hierarchy for Memory is shown below.\n",
      "\n",
      "` class Hierarchy  Memory\n",
      "\n",
      "BaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory\n",
      "\n",
      "Main helpers:\n",
      "\n",
      "class BaseChatMessageHistory\n",
      "\n",
      "Class hierarchy for Chat Message History:\n",
      "\n",
      ".. code-block::\n",
      "\n",
      "    BaseChatMessageHistory --> <name>ChatMessageHistory  # Example: ZepChatMessageHistory\n",
      "\n",
      "Main helpers:\n",
      "\n",
      "class AIMessage, BaseMessage, HumanMessage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "-> **Question**: What classes are derived from the Chain class? \n",
      "\n",
      "**189.13 secs**\n",
      "\n",
      "**Answer**:  I don't know. The code snippet provided doesn't include the definition of the Chat Message History class, so I can't determine the hierarchy without further context. \n",
      "\n",
      "-> **Question**: What kind of retrievers does LangChain have? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferenceQA(cTransformers_llm, gpt_vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745aab7f-dd44-41d0-af57-f85e734b5eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceQA(cTransformers_llm, llama2_vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c47b1e-176d-43a4-9ac7-e10b7001603b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
