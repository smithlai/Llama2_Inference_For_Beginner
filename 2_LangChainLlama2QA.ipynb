{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cf1913-705d-498b-baf3-314b8acec763",
   "metadata": {},
   "source": [
    "# QA using a Retriever\n",
    "https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b875b1dd-6145-4dad-bf2d-3c6f3381d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/f4/2e/0adf6e264b996e263b1c57cad6560ffd5492a69beb9fd779ed0463d486bc/tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /app/venv/lib/python3.10/site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in /app/venv/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /app/venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /app/venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /app/venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /app/venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.1\n"
     ]
    }
   ],
   "source": [
    "###### chromadb is not compatible with latest pydantic and fastapi\n",
    "# pydantic.errors.PydanticImportError: `BaseSettings` has been moved to the `pydantic-settings` package. See https://docs.pydantic.dev/2.3/migration/#basesettings-has-moved-to-pydantic-settings for more details.\n",
    "# !pip install chromadb\n",
    "# !pip install fastapi==0.99.1\n",
    "# !pip install pydantic==1.10.0\n",
    "# NOTE: you have to restart jupyter kernel to reload lib\n",
    "\n",
    "!pip install tiktoken  # for OpenAIEmbeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe594bd9-b409-4751-a7c3-9714163e124e",
   "metadata": {},
   "source": [
    "## Example 1: Basic QA with OpenAI\n",
    "https://python.langchain.com/docs/use_cases/question_answering/\n",
    "\n",
    "1. Load: Specify a DocumentLoader to load in your unstructured data as Documents. A Document is a piece of text (the page_content) and associated metadata.\n",
    "   There are many [Loaders](https://integrations.langchain.com/)\n",
    "   `documents = TextLoader(....).load()`  \n",
    "2. Split: Split the Document into chunks for embedding and vector storage.  \n",
    "   `texts = CharacterTextSplitter(...).split_documents(documents)`\n",
    "3. Store: To be able to look up our document splits, we first need to store them where we can later look them up.  \n",
    "   The most common way to do this is to embed the contents of each document then store the embedding and document in a vector store,  \n",
    "   with the embedding being used to index the document.  \n",
    "   `vectorstore = FAISS.from_documents(texts, embeddings)`\n",
    "\n",
    "You can find the resources [here](https://integrations.langchain.com/)\n",
    "loader: such as TextLoader\n",
    "embeddings: such as OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "vectorstore: such as FAISS and Chromadb\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18fe2bf0-6f36-447a-a922-6eaff52b556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "\n",
    "loader = TextLoader(os.path.abspath(\"./mydata/state_of_the_union.txt\"), encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "openai_api_key=\"\"\n",
    "with open('./mydata/openai_api_key.txt', 'r') as file:\n",
    "    openai_api_key = file.read().strip()\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "# docsearch = Chroma.from_documents(texts, embeddings) # Chromadb is bugged\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1986c6cd-3f93-46b7-9a6d-5995dcfbf022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The President said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, from a family of public school educators and police officers, a consensus builder, and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4b8fb-dd5e-4d71-8977-3a61eeb2f198",
   "metadata": {},
   "source": [
    "## Example2: QA with other LLM (Llama2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3273d-4218-482a-ac2f-b111901e3ef2",
   "metadata": {},
   "source": [
    "### Step 1: \n",
    "Here we will also make QA step by step  \n",
    "https://python.langchain.com/docs/use_cases/question_answering/\n",
    "\n",
    "**Load**: Specify a DocumentLoader to load in your unstructured data as Documents. A Document is a piece of text (the page_content) and associated metadata.\n",
    "There are many [Loaders](https://integrations.langchain.com/)\n",
    "`documents = TextLoader(....).load()` \n",
    "\n",
    "**Split**: Split the Document into chunks for embedding and vector storage.  \n",
    "`texts = RecursiveCharacterTextSplitter(...).split_documents(documents)`\n",
    "\n",
    "**Store**: To be able to look up our document splits, we first need to store them where we can later look them up.  \n",
    "   The most common way to do this is to embed the contents of each document then store the embedding and document in a vector store,  \n",
    "   with the embedding being used to index the document.  \n",
    "   `vectorstore = FAISS.from_documents(texts, embeddings)`\n",
    "\n",
    "In this example we will know what we will get from **VectorStore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57209bfa-a229-4144-87f4-61e0b9eeb2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What will we get from vectorestore with: \"What did the president say about Ketanji Brown Jackson\"\n",
      "===============================\n",
      "In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \n",
      "\n",
      "We cannot let this happen. \n",
      "\n",
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "from mylib.MyModelUtils import MyModelUtils\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "# 1. Load data\n",
    "def get_documents():\n",
    "    from langchain.document_loaders import TextLoader\n",
    "    loader = TextLoader(os.path.abspath(\"./mydata/state_of_the_union.txt\"), encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "# 2. Split data\n",
    "def get_text_splits(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    # text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    all_text_splits = text_splitter.split_documents(documents)\n",
    "    return all_text_splits\n",
    "\n",
    "# embedding data\n",
    "def get_sentence_transformer():\n",
    "    model_name = os.path.abspath(\"./models/sentence-transformers/all-mpnet-base-v2\")\n",
    "    model_kwargs = {\"device\": MyModelUtils.device()}\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "    return embeddings\n",
    "\n",
    "# Store data\n",
    "# type = faiss or chroma\n",
    "def get_vectorstore(all_text_splits, sentence_transformer, type=\"faiss\"):\n",
    "    # embedding and storing text splits in the vectorstore\n",
    "    if type==\"chroma\":\n",
    "        vectorstore = Chroma.from_documents(all_text_splits, sentence_transformer)\n",
    "    else: # elif type==\"faiss\":\n",
    "        vectorstore = FAISS.from_documents(all_text_splits, sentence_transformer)\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "## You can found what we get from vectorstore and will be sent to LLM\n",
    "def test_vectorstore(vectorstore):\n",
    "    query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "    docs = vectorstore.similarity_search(query)\n",
    "    # __OR__\n",
    "    # embedding_vector = OpenAIEmbeddings().embed_query(query)\n",
    "    # docs = db.similarity_search_by_vector(embedding_vector)\n",
    "    # __OR__\n",
    "    # docs_and_scores = db.similarity_search_with_score(query)\n",
    "    print(f\"What will we get from vectorestore with: \\\"{query}\\\"\")\n",
    "    print(\"===============================\")\n",
    "    print(docs[0].page_content)\n",
    "    print(\"===============================\")\n",
    "        \n",
    "documents = get_documents() # 1. load document\n",
    "all_text_splits=get_text_splits(documents) # 2. Split document\n",
    "sentence_transformer = get_sentence_transformer() # transfermer for embedding\n",
    "\n",
    "# This what we want\n",
    "vectorstore = get_vectorstore(all_text_splits, sentence_transformer) # 3. Split document\n",
    "\n",
    "test_vectorstore(vectorstore) # test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a077d0-237d-4f38-827c-2535d5d4bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also save and load FIASS\n",
    "# vectorstore.save_local(\"faiss_index\")\n",
    "# vectorstore = FAISS.load_local(\"faiss_index\", sentence_transformer)\n",
    "# test_vectorstore(vectorstore)\n",
    "\n",
    "## We can also merge 2 vectorstores\n",
    "# vectorstore1.merge_from(vectorstore2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3355f8be-2d07-4b4a-8b23-697db95c0073",
   "metadata": {},
   "source": [
    "# Step2. Chain the doc data in to QA\n",
    "我們先介紹一下 load_qa_chain，他是所有QA相關功能的核心，後續介紹的 RetrievalQA 跟 ConversationalRetrievalChain都是以他為基礎。\n",
    "以下是load_qa_chain中四種型態的一種(stuff, map_reduce, refine, map_rerank)。\n",
    "他們唯一的不同點就是對於資料來源的合併方式，稍後說明。\n",
    "模型返回具体答案。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f9e9a-a5e8-4e9a-837a-edf0548c8a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain  # :StuffDocumentsChain :BaseCombineDocumentsChain\n",
    "query = \"What did the president say about Ketanji Brown Jackson, and how old is he?\"\n",
    "docs = vectorstore.similarity_search(query) #這是我們之前的範例，可以知道我們從vertorstore找到什麼資料\n",
    "print(docs)\n",
    "doc_chain = load_qa_chain(llm=langchain_hf_llm, chain_type=\"stuff\")\n",
    "print(\"===============\")\n",
    "result = doc_chain.run(input_documents=docs, question=query) \n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0572990-fb63-4390-8a6d-33ae0f97a3ca",
   "metadata": {},
   "source": [
    "### load_qa_chain 說明\n",
    "看完上面範例後，以下是StuffDocumentsChain的[說明](https://github.com/langchain-ai/langchain/blob/740eafe41da7317f42387bdfe6d0f1f521f2cafd/libs/langchain/langchain/chains/combine_documents/stuff.py#L20)\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "    This chain takes a list of documents and first combines them into a single string.\n",
    "    It does this by formatting each document into a string with the `document_prompt`\n",
    "    and then joining them together with `document_separator`. It then adds that new\n",
    "    string to the inputs with the variable name set by `document_variable_name`.\n",
    "    Those inputs are then passed to the `llm_chain`.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "可得知 load_qa_chain(....).run(input_documents=docs, question=query)得到的StuffDocumentsChain就是把所有doc的文字內容(page_content)用\"\\n\\n\"join成一個字串，然後再放進一個prompt丟給LLM，格式如下\n",
    "\n",
    "*附錄*\n",
    "```python\n",
    "# stuff\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "於是整個QA的流程如下:    \n",
    "\n",
    "| Vector Store | load_qa_chain |\n",
    "|--------------|---------------|\n",
    "| DATA -> Loader -> Split -> Transformer + Embedding -> VectorStore -> Array(Document(),Document(), ....) | -> StuffDocumentsChain -> String -> Prompt -> LLM |    \n",
    "\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "[chain_type](https://liaokong.gitbook.io/llm-kai-fa-jiao-cheng/) 說明:  \n",
    "- stuff：(default)這種方式最簡單粗暴，會把所有的文檔一次全部傳給LLM模型進行處理2。\n",
    "    map_reduce：這種方式會先對每個文檔進行處理（映射步驟），然後再將所有處理過的文檔組合起來得到最終的輸出（減少步驟）。\n",
    "  \n",
    "- refine: 这种方式会先总结第一个 document，然后在将第一个 document 总结出的内容和第二个 document 一起发给 llm 模型在进行总结，以此类推。这种方式的好处就是在总结后一个 document 的时候，会带着前一个的 document 进行总结，给需要总结的 document 添加了上下文，增加了总结内容的连贯性。\n",
    "\n",
    "- map_rerank: 这种一般不会用在总结的 chain 上，而是会用在问答的 chain 上，他其实是一种搜索答案的匹配方式。首先你要给出一个问题，他会根据问题给每个 document 计算一个这个 document 能回答这个问题的概率分数，然后找到分数最高的那个 document ，在通过把这个 document 转化为问题的 prompt 的一部分（问题+document）发送给 llm 模型，最后 llm 模型返回具体答案。 模型返回具体答案。\n",
    "```r:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5bff9-c655-4981-9957-549aafa9f279",
   "metadata": {},
   "source": [
    "### RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16cca782-078e-4ae5-9891-20b898181a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fd102699f142b7b339cf725896fd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function langchain_pipeline_init_timeit() {} Took 36.6855 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/app/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The president said that he nominated Ketanji Brown Jackson to the United States Supreme Court.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mylib.Utils import timeit, myprint\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "model_id=os.path.abspath('./models/Llama-2-7b-chat-hf')\n",
    "model_util = MyModelUtils(model_id = model_id)\n",
    "\n",
    "@timeit\n",
    "def langchain_pipeline_init_timeit():\n",
    "    modelcfg_kwargs = model_util.default_modelconf_kwargs \n",
    "    pretrained_kwargs = model_util.make_model_kwargs_for_pretrained(**modelcfg_kwargs)\n",
    "    hf_pipeline = model_util.init_hf_pipeline(pretrained_kwargs)\n",
    "    \n",
    "    langchain_hf_llm = HuggingFacePipeline(pipeline=hf_pipeline,\n",
    "                            pipeline_kwargs={'batch_size':128},\n",
    "                         )\n",
    "    return langchain_hf_llm\n",
    "\n",
    "langchain_hf_llm = langchain_pipeline_init_timeit()\n",
    "\n",
    "# input_key: str = \"query\"  #: :meta private:\n",
    "# output_key: str = \"result\"  #: :meta private:\n",
    "qa = RetrievalQA.from_chain_type(llm=langchain_hf_llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e84711d-672f-46eb-a3c6-029a0172e79f",
   "metadata": {},
   "source": [
    "### Step2. chain the vectorstore data into llm\n",
    "\n",
    "We will do two things here:\n",
    "1. Load Language Model for Langchain (langchain.llms.HuggingFacePipeline)\n",
    "2. chain the vectorstore data into llm (ConversationalRetrievalChain)\n",
    "\n",
    "ConversationalRetrievalChain 可以視為先前 RetrievalQA 的進階版(多了歷史紀錄)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "655f71a0-234d-49f4-971d-a4e56b94283f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2ff6b26f8646f8b5d97c000ebbd064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function langchain_pipeline_init_timeit() {} Took 79.1457 seconds\n",
      "{'question': 'What did the president say about Ketanji Brown Jackson?\\n', 'chat_history': [], 'answer': \" The president mentioned Ketanji Brown Jackson in the context of nominating someone to serve on the United States Supreme Court. The president stated that he nominated Jackson 4 days ago and that she is one of our nation's top legal minds who will continue Justice Breyer's legacy of excellence.\", 'source_documents': [Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': '/app/project/mydata/state_of_the_union.txt'}), Document(page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.', metadata={'source': '/app/project/mydata/state_of_the_union.txt'}), Document(page_content='Vice President Harris and I ran for office with a new economic vision for America. \\n\\nInvest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up  \\nand the middle out, not from the top down.  \\n\\nBecause we know that when the middle class grows, the poor have a ladder up and the wealthy do very well. \\n\\nAmerica used to have the best roads, bridges, and airports on Earth. \\n\\nNow our infrastructure is ranked 13th in the world. \\n\\nWe won’t be able to compete for the jobs of the 21st Century if we don’t fix that. \\n\\nThat’s why it was so important to pass the Bipartisan Infrastructure Law—the most sweeping investment to rebuild America in history. \\n\\nThis was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen. \\n\\nWe’re done talking about infrastructure weeks. \\n\\nWe’re going to have an infrastructure decade.', metadata={'source': '/app/project/mydata/state_of_the_union.txt'}), Document(page_content='And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \\n\\nFirst, beat the opioid epidemic.', metadata={'source': '/app/project/mydata/state_of_the_union.txt'})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import os\n",
    "from mylib.MyModelUtils import MyModelUtils\n",
    "from mylib.Utils import timeit, myprint\n",
    "\n",
    "model_id=os.path.abspath('./models/Llama-2-7b-chat-hf')\n",
    "model_util = MyModelUtils(model_id = model_id)\n",
    "\n",
    "@timeit\n",
    "def langchain_pipeline_init_timeit():\n",
    "    modelcfg_kwargs = model_util.default_modelconf_kwargs \n",
    "    pretrained_kwargs = model_util.make_model_kwargs_for_pretrained(**modelcfg_kwargs)\n",
    "    hf_pipeline = model_util.init_hf_pipeline(pretrained_kwargs)\n",
    "    \n",
    "    langchain_hf_llm = HuggingFacePipeline(pipeline=hf_pipeline,\n",
    "                            pipeline_kwargs={'batch_size':128},\n",
    "                         )\n",
    "    return langchain_hf_llm\n",
    "\n",
    "langchain_hf_llm = langchain_pipeline_init_timeit()\n",
    "\n",
    "retrieval_chain = ConversationalRetrievalChain.from_llm(llm=langchain_hf_llm, retriever=vectorstore.as_retriever(), return_source_documents=True)\n",
    "query = \"What did the president say about Ketanji Brown Jackson?\\n\"\n",
    "result = retrieval_chain({\"question\": query, \"chat_history\": []})\n",
    "\n",
    "print(result['answer'])\n",
    "print(\"======================\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bae20-4f00-4c94-8bac-f1306bccaa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "好了，現在各位應該覺得很神奇，但是仔細一想又滿頭問號。\n",
    "問題來了，為什麼他知道要如何提取我們的問題給LLM，又是如何給LLM的？\n",
    "為什麼要傳入({\"question\": query, \"chat_history\": []}\n",
    "為什麼是\"question\"?\n",
    "首先我們看\n",
    "https://api.python.langchain.com/en/latest/_modules/langchain/chains/conversational_retrieval/base.html#ConversationalRetrievalChain  \n",
    "裡面有幾行程式碼:  \n",
    "```python\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "# .....\n",
    "# CONDENSE_QUESTION_PROMPT 被設為預設值\n",
    "condense_question_prompt: BasePromptTemplate = CONDENSE_QUESTION_PROMPT,\n",
    "# .....\n",
    "# \n",
    "doc_chain = load_qa_chain(\n",
    "            llm,\n",
    "            chain_type=chain_type,\n",
    "            verbose=verbose,\n",
    "            callbacks=callbacks,\n",
    "            **combine_docs_chain_kwargs,\n",
    "        )\n",
    "# ......\n",
    "condense_question_chain = LLMChain(\n",
    "    llm=_llm,\n",
    "    prompt=condense_question_prompt,\n",
    "    verbose=verbose,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "請看以下程式碼：\n",
    "\n",
    "https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/conversational_retrieval/prompts.py\n",
    "```py\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "```\n",
    "懂了嗎？\n",
    "ConversationalRetrievalChain\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
