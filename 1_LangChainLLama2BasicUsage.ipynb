{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a734497c-6379-4d65-a468-e3921f04fe13",
   "metadata": {},
   "source": [
    "# LangChain + Llama2 Basic usage\n",
    "https://python.langchain.com/docs/get_started/quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df97204e-0622-4ad2-b3f4-9f442208c45d",
   "metadata": {},
   "source": [
    "## Example 1: ChatGPT Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd1295-24f5-4394-93cf-3ade8f9d53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai_api_key=\"<OpenAI-KEY>\"\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "chat_model = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bbdf3-40b6-4ab9-b21e-dae545f462c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mylib.Utils import myprint\n",
    "text = \"hi\"\n",
    "myprint(\"-----LLM predict-------\")\n",
    "myprint(llm.predict(text))\n",
    "myprint(\"-----CHAT LLM predict-------\")\n",
    "myprint(chat_model.predict(text))\n",
    "\n",
    "from langchain.schema import HumanMessage\n",
    "messages = [HumanMessage(content=text)]\n",
    "myprint(\"-----LLM predict_message-------\")\n",
    "myprint(llm.predict_messages(messages))\n",
    "myprint(\"-----CHAT LLM predict_message-------\")\n",
    "myprint(repr(chat_model.predict_messages(messages)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0624d8-de67-4355-bb60-256740d126b4",
   "metadata": {},
   "source": [
    "## Example 2: Custom Model Usage (HuggingFace Model)\n",
    "Note: please refer to ./lib/MyModelUtils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907a6e78-b66b-4704-8c0c-3506dd9d2b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a69885c71724bfc866b78a40891dd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function hf_model_load_timeit() {} Took 5.8110 seconds\n",
      "cuda:0\n",
      "'<s> Explain me the difference between Data Lakehouse and Data Warehouse. Unterscheidung between Data Lakehouse and Data Warehouse.\\n\\nA data warehouse is a centralized repository that stores data in a structured and organized manner, making it easily accessible and queryable. On the other hand, a data lakehouse is a repository that stores data in its raw and unprocessed form, allowing for flexible and efficient querying and analysis of large datasets.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "Function hf_model_generate_timeit(LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "), LlamaTokenizerFast(name_or_path='/app/project/models/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)) {} Took 23.0388 seconds\n"
     ]
    }
   ],
   "source": [
    "## Test model \n",
    "from mylib.MyModelUtils import MyModelUtils\n",
    "from mylib.Utils import timeit, myprint\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "model_id=os.path.abspath('./models/Llama-2-7b-chat-hf')\n",
    "model_util = MyModelUtils(model_id = model_id)\n",
    "\n",
    "\n",
    "@timeit\n",
    "def hf_model_load_timeit():\n",
    "    # 1. Load and modify default modelcfg from llm file \n",
    "    #    modelcfg=transformers.AutoConfig.from_pretrained(\"modelname\",.....)\n",
    "    modelcfg_kwargs = model_util.default_modelconf_kwargs \n",
    "    pretrained_kwargs = model_util.make_model_kwargs_for_pretrained(**modelcfg_kwargs)\n",
    "    # 2. Load llm \n",
    "    #    llm = transformers.AutoModelForCausalLM.from_pretrained(\"modelname\",...,config=modelcfg,....),\n",
    "    hf_model = model_util.init_model(**pretrained_kwargs)\n",
    "    # 3. Load tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name_or_path = model_id, **pretrained_kwargs)\n",
    "    return hf_model, tokenizer\n",
    "\n",
    "hf_model, tokenizer=hf_model_load_timeit()\n",
    "\n",
    "# Actually run the thing\n",
    "@timeit\n",
    "def hf_model_generate_timeit(hf_model, tokenizer):\n",
    "    prompt = \"Explain me the difference between Data Lakehouse and Data Warehouse.\"\n",
    "    # Pass the prompt to the tokenizer\n",
    "    print(hf_model.device)\n",
    "    # 3. tokenize the inputs to pytorch\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(hf_model.device)\n",
    "    # 4. input -> hf_model -> output\n",
    "    output = hf_model.generate(**inputs, use_cache=True, max_new_tokens=256)\n",
    "    # print(output)\n",
    "    myprint(tokenizer.decode(output[0], skip_special_tokens=False))\n",
    "\n",
    "hf_model_generate_timeit(hf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512690ea-ad73-4c86-8976-5d9e6e77e7c0",
   "metadata": {},
   "source": [
    "## Example 3: Custom Model Usage (HuggingFace pipeline)\n",
    "Note: please refer to ./lib/MyModelUtils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a54737fe-f1b6-4b62-a688-94a0b88e2b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb690de736344baa054792390f5ce39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function hf_pipeline_init_timeit() {} Took 5.4456 seconds\n",
      "Function hf_pipeline_timeit() {} Took 7.9015 seconds\n",
      "[{'generated_text': \"Explain me the difference between Data Lakehouse and Data Warehouse. Unterscheidung between Data Lakehouse and Data Warehouse. A data lakehouse is a centralized repository that stores all of an organization's data, both structured and unstructured, in a single location. A data warehouse, on the other hand, is a repository that stores data in a structured format, typically in a relational database management system (RDBMS).\\n\\n\\n\\n\"}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "from mylib.MyModelUtils import MyModelUtils\n",
    "from mylib.Utils import timeit, myprint\n",
    "import transformers\n",
    "model_id=os.path.abspath('./models/Llama-2-7b-chat-hf')\n",
    "model_util = MyModelUtils(model_id = model_id)\n",
    "\n",
    "@timeit\n",
    "def hf_pipeline_init_timeit():\n",
    "    modelcfg_kwargs = model_util.default_modelconf_kwargs \n",
    "    pretrained_kwargs = model_util.make_model_kwargs_for_pretrained(**modelcfg_kwargs)\n",
    "    return model_util.init_hf_pipeline(pretrained_kwargs)\n",
    "hf_pipeline = hf_pipeline_init_timeit()\n",
    "\n",
    "@timeit\n",
    "def hf_pipeline_timeit():\n",
    "    return hf_pipeline(\"Explain me the difference between Data Lakehouse and Data Warehouse.\")\n",
    "    \n",
    "res = hf_pipeline_timeit()\n",
    "myprint(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3da049-884e-4b88-aa36-01b8dafab294",
   "metadata": {},
   "source": [
    "## Example 4: Custom Model Usage (LangChain HuggingFace pipeline)\n",
    "Note: please refer to ./lib/MyModelUtils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbcddf-aa73-45de-b9ac-dcf7ca14443a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e020fe6463774328ae28b08c4f38e46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function langchain_pipeline_init_timeit() {} Took 5.6001 seconds\n",
      "Function langchain_pipeline_call_timeit() {} Took 7.8995 seconds\n",
      "\" Unterscheidung between Data Lakehouse and Data Warehouse. A data lakehouse is a centralized repository that stores all of an organization's data, both structured and unstructured, in a single location. A data warehouse, on the other hand, is a repository that stores data in a structured format, typically in a relational database management system (RDBMS).\\n\\n\\n\\n\"\n",
      "-----langchain predict-------\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import HumanMessage\n",
    "from mylib.MyModelUtils import MyModelUtils\n",
    "from mylib.Utils import timeit, myprint\n",
    "model_id=os.path.abspath('./models/Llama-2-7b-chat-hf')\n",
    "model_util = MyModelUtils(model_id = model_id)\n",
    "\n",
    "@timeit\n",
    "def langchain_pipeline_init_timeit():\n",
    "    modelcfg_kwargs = model_util.default_modelconf_kwargs \n",
    "    pretrained_kwargs = model_util.make_model_kwargs_for_pretrained(**modelcfg_kwargs)\n",
    "    hf_pipeline = model_util.init_hf_pipeline(pretrained_kwargs)\n",
    "    \n",
    "    langchain_hf_llm = HuggingFacePipeline(pipeline=hf_pipeline,\n",
    "                            pipeline_kwargs={'batch_size':128},\n",
    "                         )\n",
    "    return langchain_hf_llm\n",
    "\n",
    "langchain_hf_llm = langchain_pipeline_init_timeit()\n",
    "\n",
    "text = \"Explain me the difference between Data Lakehouse and Data Warehouse.\"\n",
    "\n",
    "@timeit\n",
    "def langchain_pipeline_call_timeit():\n",
    "    return langchain_hf_llm(prompt=text)\n",
    "\n",
    "myprint(langchain_pipeline_call_timeit())\n",
    "\n",
    "@timeit\n",
    "def langchain_pipeline_predict_timeit():\n",
    "    print(\"-----langchain predict-------\")\n",
    "    print(langchain_hf_llm.predict(text))\n",
    "\n",
    "langchain_pipeline_predict_timeit()\n",
    "\n",
    "@timeit\n",
    "def langchain_pipeline_predict_message_timeit():\n",
    "    print(\"-----langchain predict_message-------\")\n",
    "    messages = [HumanMessage(content=text)]\n",
    "    print(langchain_hf_llm.predict_messages(messages))\n",
    "langchain_pipeline_predict_message_timeit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cf51f-7c33-42ab-be41-26ebdd794dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
