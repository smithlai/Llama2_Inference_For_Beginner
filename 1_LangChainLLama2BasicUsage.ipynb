{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a734497c-6379-4d65-a468-e3921f04fe13",
   "metadata": {},
   "source": [
    "# LangChain + Llama2 Basic usage\n",
    "https://www.readfog.com/a/1706407494707941376\n",
    "https://python.langchain.com/docs/get_started/quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df97204e-0622-4ad2-b3f4-9f442208c45d",
   "metadata": {},
   "source": [
    "## Example 1: ChatGPT Demo\n",
    "這裡我們要表達的是，LangChain的一個重要部分:Model  \n",
    "使用Model可以將你想要的LLM包起來，然後使用LangChain的API呼叫  \n",
    "[llm](https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/llms) 跟 [chat_models](https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/chat_models)可以看到預先定義好的Model package  \n",
    "\n",
    "比如呼叫OpenAI時可以用以下方法：  \n",
    "`from langchain.llms import OpenAI`  \n",
    "`from langchain.chat_models import ChatOpenAI`  \n",
    "\n",
    "客製化的model(比如Llama2)，我們會在中間包一層HuggingFace pipeline作為中間層，由HuggingFace來處理  \n",
    "`from langchain.llms import HuggingFacePipeline`  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6fd1295-24f5-4394-93cf-3ade8f9d53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai_api_key=\"\"\n",
    "with open('./mydata/openai_api_key.txt', 'r') as file:\n",
    "    openai_api_key = file.read().strip()\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "chat_model = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8bbdf3-40b6-4ab9-b21e-dae545f462c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'-----LLM predict-------'\n",
      "'_score\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
      "'-----CHAT LLM predict-------'\n",
      "'Hello! How can I assist you today?'\n",
      "'-----LLM predict_message-------'\n",
      "AIMessage(content='\\n\\nBot: Hi there! How can I help you?', additional_kwargs={}, example=False)\n",
      "'-----CHAT LLM predict_message-------'\n",
      "\"AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False)\"\n"
     ]
    }
   ],
   "source": [
    "from mylib.Utils import myprint\n",
    "text = \"hi\"\n",
    "myprint(\"-----LLM predict-------\")\n",
    "myprint(llm.predict(text))\n",
    "myprint(\"-----CHAT LLM predict-------\")\n",
    "myprint(chat_model.predict(text))\n",
    "\n",
    "from langchain.schema import HumanMessage\n",
    "messages = [HumanMessage(content=text)]\n",
    "myprint(\"-----LLM predict_message-------\")\n",
    "myprint(llm.predict_messages(messages))\n",
    "myprint(\"-----CHAT LLM predict_message-------\")\n",
    "myprint(repr(chat_model.predict_messages(messages)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0624d8-de67-4355-bb60-256740d126b4",
   "metadata": {},
   "source": [
    "## Example 2: Custom Model Usage (HuggingFace Model)\n",
    "Note: please refer to ./lib/MyModelUtils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "907a6e78-b66b-4704-8c0c-3506dd9d2b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c20164127114a8ebc31806c62fb31a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/app/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/app/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/app/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function hf_model_load_timeit() {} Took 15.8241 seconds\n",
      "cuda:0\n",
      "'<s> Explain me the difference between Data Lakehouse and Data Warehouse. Unterscheidung between Data Lakehouse and Data Warehouse.\\n\\nA data warehouse is a centralized repository that stores data in a structured and organized manner, making it easily accessible and queryable. On the other hand, a data lakehouse is a repository that stores data in its raw and unprocessed form, allowing for flexible and efficient querying and analysis of large datasets.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "Function hf_model_generate_timeit(LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "), LlamaTokenizerFast(name_or_path='/app/project/models/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)) {} Took 23.7885 seconds\n"
     ]
    }
   ],
   "source": [
    "## Test model \n",
    "from mylib.MyModelUtils import MyModelUtils\n",
    "from mylib.Utils import timeit, myprint\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "model_id=os.path.abspath('./models/Llama-2-7b-chat-hf')\n",
    "model_util = MyModelUtils(model_id = model_id)\n",
    "\n",
    "\n",
    "@timeit\n",
    "def hf_model_load_timeit():\n",
    "    # 1. Load and modify default modelcfg from llm file \n",
    "    #    modelcfg=transformers.AutoConfig.from_pretrained(\"modelname\",.....)\n",
    "    modelcfg_kwargs = model_util.default_modelconf_kwargs \n",
    "    pretrained_kwargs = model_util.make_model_kwargs_for_pretrained(**modelcfg_kwargs)\n",
    "    # 2. Load llm \n",
    "    #    llm = transformers.AutoModelForCausalLM.from_pretrained(\"modelname\",...,config=modelcfg,....),\n",
    "    hf_model = model_util.init_model(**pretrained_kwargs)\n",
    "    # 3. Load tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name_or_path = model_id, **pretrained_kwargs)\n",
    "    return hf_model, tokenizer\n",
    "\n",
    "hf_model, tokenizer=hf_model_load_timeit()\n",
    "\n",
    "# Actually run the thing\n",
    "@timeit\n",
    "def hf_model_generate_timeit(hf_model, tokenizer):\n",
    "    prompt = \"Explain me the difference between Data Lakehouse and Data Warehouse.\"\n",
    "    # Pass the prompt to the tokenizer\n",
    "    print(hf_model.device)\n",
    "    # 3. tokenize the inputs to pytorch\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(hf_model.device)\n",
    "    # 4. input -> hf_model -> output\n",
    "    output = hf_model.generate(**inputs, use_cache=True, max_new_tokens=256)\n",
    "    # print(output)\n",
    "    myprint(tokenizer.decode(output[0], skip_special_tokens=False))\n",
    "\n",
    "hf_model_generate_timeit(hf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512690ea-ad73-4c86-8976-5d9e6e77e7c0",
   "metadata": {},
   "source": [
    "## Example 3: Custom Model Usage (HuggingFace pipeline)\n",
    "Note: please refer to ./lib/MyModelUtils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54737fe-f1b6-4b62-a688-94a0b88e2b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1b961ff61b49259fb7e6b92800375c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function hf_pipeline_init_timeit() {} Took 11.9773 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/app/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function hf_pipeline_timeit() {} Took 7.9374 seconds\n",
      "[{'generated_text': \"Explain me the difference between Data Lakehouse and Data Warehouse. Unterscheidung between Data Lakehouse and Data Warehouse. A data lakehouse is a centralized repository that stores all of an organization's data, both structured and unstructured, in a single location. A data warehouse, on the other hand, is a repository that stores data in a structured format, typically in a relational database management system (RDBMS).\\n\\n\\n\\n\"}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "from mylib.MyModelUtils import MyModelUtils\n",
    "from mylib.Utils import timeit, myprint\n",
    "import transformers\n",
    "model_id=os.path.abspath('./models/Llama-2-7b-chat-hf')\n",
    "model_util = MyModelUtils(model_id = model_id)\n",
    "\n",
    "@timeit\n",
    "def hf_pipeline_init_timeit():\n",
    "    modelcfg_kwargs = model_util.default_modelconf_kwargs \n",
    "    pretrained_kwargs = model_util.make_model_kwargs_for_pretrained(**modelcfg_kwargs)\n",
    "    return model_util.init_hf_pipeline(pretrained_kwargs)\n",
    "hf_pipeline = hf_pipeline_init_timeit()\n",
    "\n",
    "@timeit\n",
    "def hf_pipeline_timeit():\n",
    "    return hf_pipeline(\"Explain me the difference between Data Lakehouse and Data Warehouse.\")\n",
    "    \n",
    "res = hf_pipeline_timeit()\n",
    "myprint(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3da049-884e-4b88-aa36-01b8dafab294",
   "metadata": {},
   "source": [
    "## Example 4: Custom Model Usage (LangChain HuggingFace pipeline)\n",
    "Note: please refer to ./lib/MyModelUtils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6abbcddf-aa73-45de-b9ac-dcf7ca14443a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/app/project/models/Llama-2-7b-chat-hf does not appear to have a file named config.json. Checkout 'https://huggingface.co//app/project/models/Llama-2-7b-chat-hf/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m     langchain_hf_llm \u001b[38;5;241m=\u001b[39m HuggingFacePipeline(pipeline\u001b[38;5;241m=\u001b[39mhf_pipeline,\n\u001b[1;32m     16\u001b[0m                             pipeline_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m128\u001b[39m},\n\u001b[1;32m     17\u001b[0m                          )\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m langchain_hf_llm\n\u001b[0;32m---> 20\u001b[0m langchain_hf_llm \u001b[38;5;241m=\u001b[39m \u001b[43mlangchain_pipeline_init_timeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# https://replicate.com/blog/how-to-prompt-llama#wrap-user-input-with-inst-inst-tags\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# correct_prompt=\"[INST] <<SYS>>You are a pirate <</SYS>> What's your favorite? [/INST]\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# myprint(langchain_pipeline_predict_timeit())\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@timeit\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlangchain_pipeline_predict_incorrect_timeit\u001b[39m():\n",
      "File \u001b[0;32m/app/project/mylib/Utils.py:9\u001b[0m, in \u001b[0;36mtimeit.<locals>.timeit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtimeit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      8\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m----> 9\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     11\u001b[0m     total_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m, in \u001b[0;36mlangchain_pipeline_init_timeit\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@timeit\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlangchain_pipeline_init_timeit\u001b[39m():\n\u001b[1;32m     11\u001b[0m     modelcfg_kwargs \u001b[38;5;241m=\u001b[39m model_util\u001b[38;5;241m.\u001b[39mdefault_modelconf_kwargs \n\u001b[0;32m---> 12\u001b[0m     pretrained_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_model_kwargs_for_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodelcfg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     hf_pipeline \u001b[38;5;241m=\u001b[39m model_util\u001b[38;5;241m.\u001b[39minit_hf_pipeline(pretrained_kwargs)\n\u001b[1;32m     15\u001b[0m     langchain_hf_llm \u001b[38;5;241m=\u001b[39m HuggingFacePipeline(pipeline\u001b[38;5;241m=\u001b[39mhf_pipeline,\n\u001b[1;32m     16\u001b[0m                             pipeline_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m128\u001b[39m},\n\u001b[1;32m     17\u001b[0m                          )\n",
      "File \u001b[0;32m/app/project/mylib/MyModelUtils.py:65\u001b[0m, in \u001b[0;36mMyModelUtils.make_model_kwargs_for_pretrained\u001b[0;34m(self, **model_conf_kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_model_kwargs_for_pretrained\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_conf_kwargs):\n\u001b[1;32m     63\u001b[0m     default_model_kwargs \u001b[38;5;241m=\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m { \n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_map\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m---> 65\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_modelcfg_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_conf_kwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_bnb_config(),\n\u001b[1;32m     67\u001b[0m     }\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_model_kwargs\n",
      "File \u001b[0;32m/app/project/mylib/MyModelUtils.py:35\u001b[0m, in \u001b[0;36mMyModelUtils.make_modelcfg_kwargs\u001b[0;34m(self, **modelconf_kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_modelcfg_kwargs\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodelconf_kwargs):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# https://huggingface.co/docs/transformers/v4.33.0/en/main_classes/configuration#transformers.PretrainedConfig\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     modelconf_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_modelconf_kwargs \u001b[38;5;241m|\u001b[39m modelconf_kwargs  \u001b[38;5;66;03m# NOTE: 3.9+ ONLY\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodelconf_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1023\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1021\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sanitize_code_revision(pretrained_model_name_or_path, revision, trust_remote_code)\n\u001b[0;32m-> 1023\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1025\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m/app/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:620\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    622\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/app/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:675\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m/app/venv/lib/python3.10/site-packages/transformers/utils/hub.py:400\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 400\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /app/project/models/Llama-2-7b-chat-hf does not appear to have a file named config.json. Checkout 'https://huggingface.co//app/project/models/Llama-2-7b-chat-hf/None' for available files."
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "from mylib.MyModelUtils import MyModelUtils\n",
    "from mylib.Utils import timeit, myprint\n",
    "model_id=os.path.abspath('./models/Llama-2-7b-chat-hf')\n",
    "model_util = MyModelUtils(model_id = model_id)\n",
    "\n",
    "@timeit\n",
    "def langchain_pipeline_init_timeit():\n",
    "    modelcfg_kwargs = model_util.default_modelconf_kwargs \n",
    "    pretrained_kwargs = model_util.make_model_kwargs_for_pretrained(**modelcfg_kwargs)\n",
    "    hf_pipeline = model_util.init_hf_pipeline(pretrained_kwargs)\n",
    "    \n",
    "    langchain_hf_llm = HuggingFacePipeline(pipeline=hf_pipeline,\n",
    "                            pipeline_kwargs={'batch_size':128},\n",
    "                         )\n",
    "    return langchain_hf_llm\n",
    "\n",
    "langchain_hf_llm = langchain_pipeline_init_timeit()\n",
    "\n",
    "# https://replicate.com/blog/how-to-prompt-llama#wrap-user-input-with-inst-inst-tags\n",
    "\n",
    "# correct_prompt=\"[INST] <<SYS>>You are a pirate <</SYS>> What's your favorite? [/INST]\"\n",
    "\n",
    "# @timeit\n",
    "# def langchain_pipeline_call_timeit():\n",
    "#     print(\"-----langchain __exec__-------\")\n",
    "#     return langchain_hf_llm(prompt=correct_prompt)\n",
    "\n",
    "# myprint(langchain_pipeline_call_timeit())\n",
    "\n",
    "# @timeit\n",
    "# def langchain_pipeline_predict_timeit():\n",
    "#     print(\"-----langchain predict-------\")\n",
    "#     return langchain_hf_llm.predict(correct_prompt)\n",
    "\n",
    "# myprint(langchain_pipeline_predict_timeit())\n",
    "\n",
    "\n",
    "@timeit\n",
    "def langchain_pipeline_predict_incorrect_timeit():\n",
    "    \n",
    "    print(\"-----langchain predict (incorrect)-------\")\n",
    "    incorrect=\"You are a pirate, What's your favorite?\"\n",
    "    return langchain_hf_llm.predict(incorrect)\n",
    "\n",
    "myprint(langchain_pipeline_predict_incorrect_timeit())\n",
    "\n",
    "\n",
    "@timeit\n",
    "def langchain_pipeline_predict_message_timeit():\n",
    "    print(\"-----langchain predict_message-------\")\n",
    "    messages = [SystemMessage(content=\"You are a pirate\"), HumanMessage(content=\"What's your favorite?\")]\n",
    "    print(langchain_hf_llm.predict_messages(messages))\n",
    "langchain_pipeline_predict_message_timeit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cf51f-7c33-42ab-be41-26ebdd794dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
