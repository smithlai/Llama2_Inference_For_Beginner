version: "3.3"
services:
  llama2_inference_for_beginner:
    build:
      context: ../  # The base project path. to fasten speed,we don't set to parent folder.
      dockerfile: ./docker/Dockerfile  # The Dockerfile name
      # context: ./  # The base project path. to fasten speed,we don't set to parent folder.
      # dockerfile: ./Dockerfile # The Dockerfile name
      args:
        PROJECTDIR_NAME: ${PROJECTDIR_NAME:-project_src}
        # HOST_MODEL_PATH: ${HOST_MODEL_PATH:-.}
        # HOST_LORA_PATH: ${HOST_MODEL_PATH:-.}
        # HOST_LORA_1: ${HOST_LORA_1:-.}
    #image: custom-image name
    env_file: .env
    network_mode: host
    stdin_open: true
    tty: true
    volumes:
      - ../:/app/${PROJECTDIR_NAME}  # relative path from current file (docker-compose.yml)
      # - ${HOST_MODEL_PATH}/:/app/${PROJECTDIR_NAME}/models
      # - ${HOST_LORA_PATH}/:/app/${PROJECTDIR_NAME}/loras
      
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
