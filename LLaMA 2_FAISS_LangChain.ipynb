{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7672235-960a-4a8f-bb59-767840fe3fb2",
   "metadata": {},
   "source": [
    "# Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data\n",
    "https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e25dd51-74a7-4f75-881e-dbf3b5bef00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8c553-9f39-4d3b-baee-73124d9f0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = './models/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "# hf_auth = '<add your access token here>'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    # use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    # use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d36bb5-bbaa-4e0d-b7fe-e0bf85d5324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3996c4e-376a-49c8-9efe-38ba2f441976",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d514726-c561-4510-8e7d-6f03029bd9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366e9c8-14ac-4a7b-8c71-4494a8737dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88905ce-8ca0-4598-9693-a9c85553d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # 就是是否連input也重覆輸出，langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    do_sample=True,\n",
    "    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=256,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "\n",
    "res = generate_text(\"Explain me the difference between Data Lakehouse and Data Warehouse.\")\n",
    "print(res[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2de6bb-c7e8-43aa-b50a-3bce5841a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# checking again that everything is working fine\n",
    "llm(prompt=\"Explain me the difference between Data Lakehouse and Data Warehouse.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8127e89b-58ed-4ff2-8a59-74cff7ca2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def make_faiss_chain(llm, documents):    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "    all_splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    model_kwargs = {\"device\": \"cuda\"}\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "    # storing embeddings in the vector store\n",
    "    vectorstore = FAISS.from_documents(all_splits, embeddings)\n",
    "    \n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92ed57-5570-416c-a256-73584c9757ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "web_links = [\"https://www.databricks.com/\",\"https://help.databricks.com\",\"https://databricks.com/try-databricks\",\"https://help.databricks.com/s/\",\"https://docs.databricks.com\",\"https://kb.databricks.com/\",\"http://docs.databricks.com/getting-started/index.html\",\"http://docs.databricks.com/introduction/index.html\",\"http://docs.databricks.com/getting-started/tutorials/index.html\",\"http://docs.databricks.com/release-notes/index.html\",\"http://docs.databricks.com/ingestion/index.html\",\"http://docs.databricks.com/exploratory-data-analysis/index.html\",\"http://docs.databricks.com/data-preparation/index.html\",\"http://docs.databricks.com/data-sharing/index.html\",\"http://docs.databricks.com/marketplace/index.html\",\"http://docs.databricks.com/workspace-index.html\",\"http://docs.databricks.com/machine-learning/index.html\",\"http://docs.databricks.com/sql/index.html\",\"http://docs.databricks.com/delta/index.html\",\"http://docs.databricks.com/dev-tools/index.html\",\"http://docs.databricks.com/integrations/index.html\",\"http://docs.databricks.com/administration-guide/index.html\",\"http://docs.databricks.com/security/index.html\",\"http://docs.databricks.com/data-governance/index.html\",\"http://docs.databricks.com/lakehouse-architecture/index.html\",\"http://docs.databricks.com/reference/api.html\",\"http://docs.databricks.com/resources/index.html\",\"http://docs.databricks.com/whats-coming.html\",\"http://docs.databricks.com/archive/index.html\",\"http://docs.databricks.com/lakehouse/index.html\",\"http://docs.databricks.com/getting-started/quick-start.html\",\"http://docs.databricks.com/getting-started/etl-quick-start.html\",\"http://docs.databricks.com/getting-started/lakehouse-e2e.html\",\"http://docs.databricks.com/getting-started/free-training.html\",\"http://docs.databricks.com/sql/language-manual/index.html\",\"http://docs.databricks.com/error-messages/index.html\",\"http://www.apache.org/\",\"https://databricks.com/privacy-policy\",\"https://databricks.com/terms-of-use\"] \n",
    "loader = WebBaseLoader(web_links)\n",
    "documents = loader.load()\n",
    "chain = make_faiss_chain(llm, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ef2ef-7e80-4e24-8763-9b4f335a50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_history = []\n",
    "\n",
    "query = \"What is Data lakehouse architecture in Databricks?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013240b-44ff-4339-bfe9-69d3d24b5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader, NotebookLoader\n",
    "loader = NotebookLoader(\"data.ipynb\")\n",
    "documents = loader.load()\n",
    "chain = make_faiss_chain(llm, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746d3e3-c5da-4a46-a4fc-beb47a977717",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"\\nHuman:Tell me something about Ben, and tell me some names of his collagues\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d6c48-beda-4fc4-ac5c-db773b811fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
